{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # T·ªça ƒë·ªô 3 ƒëi·ªÉm v√† gi√° tr·ªã t∆∞∆°ng ·ª©ng\n",
    "# points = np.array([[1, 2], [3, 5], [7, 6]])  # [x,y]\n",
    "# values = np.array([5, 7, 3])  # gi√° tr·ªã t·∫°i m·ªói ƒëi·ªÉm\n",
    "\n",
    "# # ƒêi·ªÉm c·∫ßn d·ª± ƒëo√°n\n",
    "# target_point = np.array([10, 4])\n",
    "\n",
    "# # T√≠nh kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm target ƒë·∫øn c√°c ƒëi·ªÉm ƒë√£ bi·∫øt\n",
    "# distances = np.sqrt(np.sum((points - target_point)**2, axis=1))\n",
    "\n",
    "# # Tr√°nh chia cho 0 n·∫øu target tr√πng v·ªõi ƒëi·ªÉm ƒë√£ bi·∫øt\n",
    "# distances = np.where(distances == 0, 1e-10, distances)\n",
    "\n",
    "# # T√≠nh tr·ªçng s·ªë (ngh·ªãch ƒë·∫£o kho·∫£ng c√°ch)\n",
    "# weights = 1 / distances\n",
    "\n",
    "# # D·ª± ƒëo√°n gi√° tr·ªã\n",
    "# predicted_value = np.sum(values * weights) / np.sum(weights)\n",
    "\n",
    "# print(f\"Gi√° tr·ªã d·ª± ƒëo√°n t·∫°i {target_point}: {predicted_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # ƒê·ªçc file CSV\n",
    "# data = pd.read_csv('Danh saÃÅch traÃ£m_20200413_222423.csv')  # Thay 'du_lieu.csv' b·∫±ng t√™n file c·ªßa b·∫°n\n",
    "\n",
    "# # Xem 5 d√≤ng ƒë·∫ßu ti√™n\n",
    "# # print(data.head())\n",
    "# data_new = data[['Kinh ƒë·ªô', 'Vƒ© ƒë·ªô', 'T√™n tr·∫°m']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004da611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ƒê·ªçc file Excel\n",
    "\n",
    "\n",
    "# # L·∫•y d·ªØ li·ªáu t·ª´ c√°c c·ªôt c·ª• th·ªÉ\n",
    "# selected_data = data[['T√™n tr·∫°m', 'Kinh ƒë·ªô', 'Vƒ© ƒë·ªô']]\n",
    "# print(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.interpolate import Rbf\n",
    "# import numpy as np\n",
    "\n",
    "# known_points = data_new[[\"Kinh ƒë·ªô\", \"Vƒ© ƒë·ªô\"]][:50].values\n",
    "# known_aqi = data[\"PM-2-5(¬µg/Nm3}\"][:50].values\n",
    "\n",
    "# # H√†m n·ªôi suy IDW\n",
    "# def idw_interpolation(target_lon, target_lat, power=2):\n",
    "#     distances = np.sqrt((known_points[:,0] - target_lon)**2 + (known_points[:,1] - target_lat)**2)\n",
    "#     weights = 1 / (distances ** power)\n",
    "#     weights[distances == 0] = 1e10  # Tr√°nh chia 0 n·∫øu tr√πng v·ªã tr√≠\n",
    "#     return np.sum(known_aqi * weights) / np.sum(weights)\n",
    "\n",
    "# # V√≠ d·ª•: T√≠nh AQI t·∫°i m·ªôt ƒëi·ªÉm m·ªõi (105.8, 21.2)\n",
    "# target_aqi = idw_interpolation(105.8, 21.2)\n",
    "# print(f\"AQI d·ª± ƒëo√°n: {target_aqi:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c863be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = {\n",
    "    \"B·∫Øc Giang_ Khu li√™n c∆° quan t·ªânh B·∫Øc Giang - P. Ng√¥ Quy·ªÅn - TP. B·∫Øc Giang\": [21.281157, 106.201736],\n",
    "    \"B√¨nh ƒê·ªãnh_ huy·ªán Tuy Ph∆∞·ªõc\": [13.856610, 109.164213],\n",
    "    \"B√¨nh ƒê·ªãnh_ Khu√¥n vi√™n C√¢y xanh g·∫ßn c·∫ßu chui ƒë∆∞·ªùng Hoa L∆∞ - TP. Quy Nh∆°n\": [13.787129, 109.215741],\n",
    "    \"B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp Th√†nh\": [10.997342, 106.648577],\n",
    "    \"Cao B·∫±ng_ Ph∆∞·ªùng ƒê·ªÅ Th√°m   TP. Cao B·∫±ng\": [22.680377, 106.210631],\n",
    "    \"Cao B·∫±ng_ TTQMT - ph∆∞·ªùng S√¥ng Hi·∫øn\": [22.663080, 106.231711],\n",
    "    \"ƒê√† N·∫µng_ 41 ƒë∆∞·ªùng L√™ Du·∫©n\": [16.071238, 108.219467],\n",
    "    \"ƒê√† N·∫µng_ Khu√¥n vi√™n tr∆∞·ªùng ƒêH s∆∞ ph·∫°m ƒê√† N·∫µng\": [16.061616, 108.159076],\n",
    "    \"ƒê√† N·∫µng_ Ph·∫°m H√πng\": [15.993340, 108.207447],\n",
    "    \"ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n\": [16.003714, 108.263151],\n",
    "    \"Gia Lai_ BQL KCN Tr√† ƒêa - Tp Pleiku \": [14.019337, 108.031749],\n",
    "    \"Gia lai_ Ch∆∞ S√™\": [13.714100, 108.066418],\n",
    "    \"Gia Lai_ TTQT TN&MT - P.Th·ªëng Nh·∫•t - TP Pleiku\": [13.973233, 107.998157],\n",
    "    \"Gia Lai_ UBND th·ªã x√£ An Kh√™\": [13.953118, 108.656818],\n",
    "    \"H√† Nam_ C√¥ng Vi√™n Nam Cao - P.Quang Trung - TP. Ph·ªß L√Ω\": [20.545756, 105.913503],\n",
    "    \"H√† N·ªôi_ 556 Nguy·ªÖn VƒÉn C·ª´\": [21.048730, 105.882471],\n",
    "    \"H√† N·ªôi_ C√¥ng vi√™n Nh√¢n Ch√≠nh - Khu·∫•t Duy Ti·∫øn\": [21.002766, 105.797385],\n",
    "    \"H√† N·ªôi_ ƒêHBK c·ªïng Parabol ƒë∆∞·ªùng Gi·∫£i Ph√≥ng\": [21.005054, 105.841675],\n",
    "    \"H√† N·ªôi_ Khu v·ª±c LƒÉng B√°c \": [21.036731, 105.834704],\n",
    "    \"H·∫£i D∆∞∆°ng_ UBND TP. H·∫£i D∆∞∆°ng - 106 ƒê∆∞·ªùng Tr·∫ßn H∆∞ng ƒê·∫°o\": [20.938148, 106.336476],\n",
    "    \"HCM_ ƒê. L√™ H·ªØu Ki·ªÅu - P. B√¨nh Tr∆∞ng T√¢y - Qu·∫≠n 2 (Ng√£ ba L√™ H·ªØu Ki·ªÉu v√† Tr∆∞∆°ng VƒÉn Bang\": [10.783304, 106.752102],\n",
    "    \"HCM_ Khu Li√™n c∆° quan B·ªô T√†i Nguy√™n v√† M√¥i Tr∆∞·ªùng - s·ªë 20 ƒê. L√Ω Ch√≠nh Th·∫Øng\": [10.782258, 106.683486],\n",
    "    \"H∆∞ng Y√™n_ s·ªë 437 Nguy·ªÖn VƒÉn Linh - Tp H∆∞ng Y√™n\": [20.661785, 106.058407],\n",
    "    \"Kh√°nh H√≤a_  Tr·∫°m kh√¥ng kh√≠ xung quanh Ninh An\": [12.564347, 109.155240],\n",
    "    \"Kh√°nh H√≤a_ ph∆∞·ªùng Vƒ©nh H√≤a - Nha Trang\": [12.298436, 109.209487],\n",
    "    \"L√¢m ƒê·ªìng_ V∆∞·ªùn hoa - ƒë·ªëi di·ªán THCS Lam S∆°n - Ph∆∞·ªùng 6 - TP ƒê√† L·∫°t\": [11.950191, 108.449910],\n",
    "    \"L·∫°ng S∆°n_ ƒê√¥ng T√¢n - H·ªØu L≈©ng\": [21.530391, 106.363766],\n",
    "    \"L·∫°ng S∆°n_ x√£ H·ªìng Phong - Cao L·ªôc\": [21.92698425708097, 106.67072321030116],\n",
    "    \"Long An_ UBND Tp T√¢n An - 76 H√πng V∆∞∆°ng - P.2\": [10.538870347238994, 106.40436138542212],\n",
    "    \"Ninh Thu·∫≠n_ C√¥ng vi√™n (b·∫øn xe c≈©) - ƒê. Th·ªëng Nh·∫•t - P. Thanh S∆°n - TP Phan Rang\": [11.57432077633915, 108.9916348174917],\n",
    "    \"Ph√∫ Th·ªç_ ƒë∆∞·ªùng H√πng V∆∞∆°ng - Tp Vi·ªát Tr√¨\": [21.373064290983997, 105.34136208557706],\n",
    "    \n",
    "    \"Qu·∫£ng B√¨nh_ Khu kinh t·∫ø H√≤n La\": [17.954712757753764, 106.504983894801],\n",
    "    \"Qu·∫£ng Nam_ Ti·∫øp gi√°p ƒê. H√πng V∆∞∆°ng - KDC ƒê. H·ªì Xu√¢n H∆∞∆°ng\": [15.58733251331769, 108.49114571230324],\n",
    "    \"Qu·∫£ng Ng√£i_ UBND P. Nguy·ªÖn Nghi√™m - TP Qu·∫£ng Ng√£i\": [15.14119051190871, 108.79939048711188],\n",
    "    \"Qu·∫£ng Ng√£i_ UBND P. Nguy·ªÖn Nghi√™m - TP Qu·∫£ng Ng√£i\": [15.122418919236212, 108.80027264597365],\n",
    "    \"Qu·∫£ng Ninh_ Km11 - Minh Th√†nh\": [21.01989025867557, 106.85143264435085],\n",
    "    \"Qu·∫£ng Ninh_ Nh√† m√°y tuy·ªÉn than Nam C·∫ßu Tr·∫Øng - H·∫° Long\": [20.95725916805329, 107.12923634683354],\n",
    "    \"Qu·∫£ng Ninh_ Nh√† vƒÉn h√≥a Y√™n M·ªπ  x√£ L√™ L·ª£i - Tp H·∫° Long \": [20.946162423982845, 106.80131064489727],\n",
    "    \"Qu·∫£ng Ninh_ Nhu·ªá H·ªï - ƒê√¥ng Tri·ªÅu\": [21.090460221058205, 106.55225715231316],\n",
    "    \"Qu·∫£ng Ninh_ UBND TP U√¥ng B√≠\": [21.047128855784653, 106.76401641780717],\n",
    "    \"T√¢y Ninh_ Th·ªã x√£ Tr·∫£ng B√†ng\": [11.103051023399988, 106.37002816153138],\n",
    "    \"Th√°i B√¨nh_ C·∫ßu Th√°i B√¨nh - ƒê. Tr·∫ßn Th√°i T√¥ng - P. B·ªì Xuy√™n - TP Th√°i B√¨nh\": [20.453461817002513, 106.33917763320316],\n",
    "    \"Th√°i Nguy√™n_ Ch·ª£ t·ªï 7 - 8 P M·ªè Ch√® - TP S√¥ng C√¥ng\": [21.475583722333706, 105.84440435007424],\n",
    "    \"Th√°i nguy√™n_ ƒê∆∞·ªùng H√πng V∆∞∆°ng - Tp Th√°i Nguy√™n\": [21.595882163215716, 105.8438805189105],\n",
    "    \"Th√°i Nguy√™n_ S√¢n v·∫≠n ƒë·ªông Gang th√©p - P Trung Th√†nh - TP Th√°i Nguy√™n\": [21.53921758920249, 105.87206418244617],\n",
    "    \"Th√°i Nguy√™n_ T·ªï 7 - Quan Tri·ªÅu - TP Th√°i Nguy√™n\": [21.58175774433606, 105.81495996333075],\n",
    "    \"Thanh Ho√°_ UBND P. Lam S∆°n  TX. B·ªâm S∆°n\": [20.076351404264194, 105.8908933493451],\n",
    "    \"Th·ª´a Thi√™n Hu·∫ø_ 83 ƒë∆∞·ªùng H√πng V∆∞∆°ng\": [16.57069210023765, 107.58904036545017],\n",
    "    \"Tr√† Vinh_ Tp. Tr√† Vinh\": [10.30219789286043, 106.28602362797926],\n",
    "    \"Tr√† Vinh_ x√£ D√¢n Th√†nh  TX Duy√™n H·∫£i\": [9.834579655459574, 106.4837318245401],\n",
    "    \"Tr√† Vinh_ x√£ ƒê√¥ng H·∫£i  huy·ªán Duy√™n H·∫£i \": [9.847489130533111, 106.41780547414956],\n",
    "    \"Vƒ©nh Long_ Khu h√†nh ch√≠nh Tx.B√¨nh Minh\": [10.48481147215296, 105.82461868577303],\n",
    "    \"Vƒ©nh Long_ UBND t·ªânh  ƒë∆∞·ªùng Ho√†ng Th√°i Hi·∫øu\": [10.62087878304751, 105.93450807937366],\n",
    "    \"V≈©ng T√†u_ ƒê. Huy·ªÅn Tr√¢n C√¥ng Ch√∫a - Ph∆∞·ªùng 8 - TP V≈©ng T√†u\": [10.718523716902757, 107.121046929018],\n",
    "    \"V≈©ng T√†u_ Ng√£ t∆∞ Gi·∫øng n∆∞·ªõc - Tp.V≈©ng T√†u\": [10.742368515382786, 107.16499899318077],\n",
    "    \"V≈©ng T√†u_ Ng√£ t∆∞ Phan ƒêƒÉng L∆∞u v√† 27_4 - Tp.B√† R·ªãa\": [10.874856149648686, 107.14303943215671],\n",
    "    \"V≈©ng T√†u_ Ti·ªÉu h·ªçc T√≥c Ti√™n - TX.Ph√∫ M·ªπ\": [11.029696049429493, 107.20897077244157],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14149cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 2. H√†m t√≠nh AQI theo PM2.5 ===\n",
    "def calculate_pm25_aqi(pm25):\n",
    "    try:\n",
    "        pm25 = float(pm25)\n",
    "    except:\n",
    "        return None\n",
    "    breakpoints = [\n",
    "        (0.0, 12.0, 0, 50),\n",
    "        (12.1, 35.4, 51, 100),\n",
    "        (35.5, 55.4, 101, 150),\n",
    "        (55.5, 150.4, 151, 200),\n",
    "        (150.5, 250.4, 201, 300),\n",
    "        (250.5, 350.4, 301, 400),\n",
    "        (350.5, 500.4, 401, 500),\n",
    "    ]\n",
    "    \n",
    "    for c_low, c_high, i_low, i_high in breakpoints:\n",
    "        if c_low <= pm25 <= c_high:\n",
    "            aqi = (i_high - i_low) / (c_high - c_low) * (pm25 - c_low) + i_low\n",
    "            return round(aqi, 1)\n",
    "    return None  # Ngo√†i ph·∫°m vi t√≠nh\n",
    "# === 1. ƒê·ªçc file Excel ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "folder_data = \"So lieu TB 1h T11.2024-T3.2025\"\n",
    "all_data = []\n",
    "for idx, file in enumerate(os.listdir(folder_data)):\n",
    "    file_path = f\"{folder_data}\\{file}\"\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    for index, value in coordinates.items():\n",
    "        if index in file:\n",
    "            try:\n",
    "                df['PM-2-5(¬µg/Nm3}'] = pd.to_numeric(df['PM-2-5(¬µg/Nm3}'], errors='coerce')\n",
    "                df['AQI_PM2.5'] = df['PM-2-5(¬µg/Nm3}'].apply(calculate_pm25_aqi)\n",
    "                df['AQI_PM2.5'] = df['AQI_PM2.5'].interpolate(method='linear')\n",
    "                selected_columns = ['Datetime', 'AQI_PM2.5']\n",
    "                df_selected = df[selected_columns].copy()\n",
    "                df_selected[\"Kinh ƒë·ªô\"] = value[0]\n",
    "                df_selected[\"Vƒ© ƒë·ªô\"] = value[1]\n",
    "                df_selected[\"T√™n\"] = index\n",
    "                print(index, value)\n",
    "                all_data.append(df_selected)\n",
    "            except:\n",
    "                continue\n",
    "    # if idx> 2:\n",
    "    #     break\n",
    "# === 7. Ghi ra file m·ªõi ===\n",
    "output_file = \"AQI_PM25_VietNam.xlsx\"\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "final_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ File k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a70563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# final_df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3801b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"Datetime\"][10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0120bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = final_df[final_df['Datetime'] == \"12:00 25/01/2025\"]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2990cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7669a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Gi·∫£ s·ª≠ b·∫°n c√≥ dataframe df v·ªõi c√°c c·ªôt 'Kinh ƒë·ªô' v√† 'Vƒ© ƒë·ªô'\n",
    "# Chuy·ªÉn t·ªça ƒë·ªô sang m·∫£ng numpy\n",
    "coordinates = filtered_df[[\"Kinh ƒë·ªô\", \"Vƒ© ƒë·ªô\"]].values\n",
    "\n",
    "# Kh·ªüi t·∫°o DBSCAN\n",
    "db = DBSCAN(eps=0.01, min_samples=3, metric='haversine')  # eps=0.01 km, min_samples=3\n",
    "\n",
    "# Ph√¢n c·ª•m d·ªØ li·ªáu t·ªça ƒë·ªô\n",
    "labels = db.fit_predict(np.radians(coordinates))  # S·ª≠ d·ª•ng b√°n k√≠nh g√≥c khi t√≠nh kho·∫£ng c√°ch\n",
    "\n",
    "# Th√™m label c·ª•m v√†o DataFrame\n",
    "filtered_df['Cluster'] = labels\n",
    "\n",
    "# Xem k·∫øt qu·∫£ ph√¢n c·ª•m\n",
    "print(filtered_df[['Kinh ƒë·ªô', 'Vƒ© ƒë·ªô', 'Cluster']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18466774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Ch·ªâ l·∫•y c√°c ƒëi·ªÉm thu·ªôc c·ª•m (kh√¥ng ph·∫£i nhi·ªÖu)\n",
    "core_samples = filtered_df[filtered_df['Cluster'] != -1][['Kinh ƒë·ªô', 'Vƒ© ƒë·ªô']].values\n",
    "core_labels = filtered_df[filtered_df['Cluster'] != -1]['Cluster'].values\n",
    "\n",
    "# Kh·ªüi t·∫°o NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=1, metric='haversine').fit(np.radians(core_samples))\n",
    "\n",
    "# T√¨m c·ª•m g·∫ßn nh·∫•t cho c√°c ƒëi·ªÉm nhi·ªÖu\n",
    "noise_points = filtered_df[filtered_df['Cluster'] == -1][['Kinh ƒë·ªô', 'Vƒ© ƒë·ªô']].values\n",
    "distances, indices = nbrs.kneighbors(np.radians(noise_points))\n",
    "\n",
    "# G√°n nh√£n c·ª•m g·∫ßn nh·∫•t\n",
    "filtered_df.loc[filtered_df['Cluster'] == -1, 'Cluster'] = core_labels[indices.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cluster_label in filtered_df['Cluster'].unique():\n",
    "#     cluster_data = filtered_df[filtered_df['Cluster'] == cluster_label]\n",
    "#     listpre = cluster_data[\"AQI_PM2.5\"]\n",
    "#     lats = cluster_data[\"Kinh ƒë·ªô\"]\n",
    "#     lons = cluster_data[\"Vƒ© ƒë·ªô\"]\n",
    "#     point = list(zip(lats, lons))\n",
    "#     print(len(lats), len(lons), len(point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_clean = final_df[['Kinh ƒë·ªô', 'Vƒ© ƒë·ªô', 'AQI_PM2.5']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2377dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "# === 1. Train Random Forest ===\n",
    "X = filtered_df_clean[['Kinh ƒë·ªô', 'Vƒ© ƒë·ªô']]\n",
    "y = filtered_df_clean['AQI_PM2.5']\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# === 2. T·∫°o l∆∞·ªõi ƒë·ªÉ d·ª± ƒëo√°n ===\n",
    "lat_min, lat_max = X['Kinh ƒë·ªô'].min()-1, X['Kinh ƒë·ªô'].max()+1\n",
    "lon_min, lon_max = X['Vƒ© ƒë·ªô'].min()-1, X['Vƒ© ƒë·ªô'].max()+1\n",
    "grid_lat = np.linspace(lat_min, lat_max, 300)\n",
    "grid_lon = np.linspace(lon_min, lon_max, 300)\n",
    "grid_lon_mesh, grid_lat_mesh = np.meshgrid(grid_lon, grid_lat)\n",
    "\n",
    "grid_points = np.c_[grid_lat_mesh.ravel(), grid_lon_mesh.ravel()]\n",
    "grid_predictions = model.predict(grid_points).reshape(grid_lat_mesh.shape)\n",
    "grid_predictions = gaussian_filter(grid_predictions, sigma=2)\n",
    "\n",
    "# === 3. V·∫Ω b·∫£n ƒë·ªì nhi·ªát ===\n",
    "aqi_colors = ['#00e400', '#ffff00', '#ff7e00', '#ff0000', '#8f3f97', '#7e0023']\n",
    "aqi_cmap = LinearSegmentedColormap.from_list(\"aqi_smooth\", aqi_colors, N=256)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(\n",
    "    grid_predictions,\n",
    "    extent=(lon_min, lon_max, lat_min, lat_max),\n",
    "    origin='lower',\n",
    "    cmap=aqi_cmap,\n",
    "    alpha=0.75,\n",
    "    vmin=0,\n",
    "    vmax=500\n",
    ")\n",
    "plt.colorbar(label=\"AQI PM2.5\")\n",
    "plt.title(\"N·ªôi suy AQI PM2.5 b·∫±ng Random Forest\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "from folium.raster_layers import ImageOverlay\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from shapely.ops import unary_union\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# ==== 1. D·ªØ li·ªáu ban ƒë·∫ßu ====\n",
    "# for idx, pre in enumerate(timelamp):\n",
    "idx = 'pre'\n",
    "vn_mainland = gpd.read_file(\"gadm41_VNM_1.json\")\n",
    "hoang_sa = gpd.read_file(\"gadm36_XSP_0.json\")\n",
    "truong_sa = gpd.read_file(\"gadm36_XPI_0.json\")\n",
    "\n",
    "# G·ªôp l·∫°i th√†nh m·ªôt GeoDataFrame\n",
    "vn_full = gpd.GeoDataFrame(pd.concat([vn_mainland, hoang_sa, truong_sa], ignore_index=True), crs='EPSG:4326')\n",
    "m = folium.Map(location=[14.0583, 108.2772], zoom_start=6, tiles='CartoDB Positron')\n",
    "\n",
    "X = filtered_df[['Kinh ƒë·ªô', 'Vƒ© ƒë·ªô']]\n",
    "y = filtered_df['AQI_PM2.5']\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# === 2. T·∫°o l∆∞·ªõi ƒë·ªÉ d·ª± ƒëo√°n ===\n",
    "lat_min, lat_max = X['Kinh ƒë·ªô'].min()-1, X['Kinh ƒë·ªô'].max()+1\n",
    "lon_min, lon_max = X['Vƒ© ƒë·ªô'].min()-1, X['Vƒ© ƒë·ªô'].max()+1\n",
    "grid_lat = np.linspace(lat_min, lat_max, 300)\n",
    "grid_lon = np.linspace(lon_min, lon_max, 300)\n",
    "grid_lon_mesh, grid_lat_mesh = np.meshgrid(grid_lon, grid_lat)\n",
    "\n",
    "grid_points = np.c_[grid_lat_mesh.ravel(), grid_lon_mesh.ravel()]\n",
    "grid_intensity = model.predict(grid_points).reshape(grid_lat_mesh.shape)\n",
    "grid_intensity = gaussian_filter(grid_predictions, sigma=2)\n",
    "\n",
    "vn_union = vn_full.union_all()  # N·ªëi t·∫•t c·∫£ v√πng l·∫°i th√†nh 1 polygon\n",
    "\n",
    "# T·∫°o mask: ƒëi·ªÉm n√†o ngo√†i bi√™n VN th√¨ ƒë·∫∑t NaN\n",
    "mask = np.full(grid_intensity.shape, False)\n",
    "\n",
    "for i in range(grid_lat_mesh.shape[0]):\n",
    "    for j in range(grid_lat_mesh.shape[1]):\n",
    "        point = Point(grid_lon_mesh[i, j], grid_lat_mesh[i, j])\n",
    "        if not vn_union.contains(point):\n",
    "            mask[i, j] = True\n",
    "\n",
    "grid_intensity_masked = np.ma.array(grid_intensity, mask=mask)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "# plt.imshow(grid_intensity_masked, extent=(lon_min, lon_max, lat_min, lat_max),\n",
    "#         origin='lower', cmap='hot', alpha=0.6)\n",
    "# Danh s√°ch m√†u chu·∫©n AQI theo th·ª© t·ª± tƒÉng d·∫ßn\n",
    "aqi_colors = [\n",
    "    '#00e400',  # Green (0‚Äì50)\n",
    "    '#ffff00',  # Yellow (51‚Äì100)\n",
    "    '#ff7e00',  # Orange (101‚Äì150)\n",
    "    '#ff0000',  # Red (151‚Äì200)\n",
    "    '#8f3f97',  # Purple (201‚Äì300)\n",
    "    '#7e0023'   # Maroon (301‚Äì500)\n",
    "]\n",
    "\n",
    "# T·∫°o colormap n·ªôi suy t·ª´ danh s√°ch tr√™n\n",
    "aqi_cmap = LinearSegmentedColormap.from_list(\"aqi_smooth\", aqi_colors, N=100)\n",
    "\n",
    "plt.imshow(\n",
    "    grid_intensity_masked,\n",
    "    extent=(lon_min, lon_max, lat_min, lat_max),\n",
    "    origin='lower',\n",
    "    cmap=aqi_cmap,     # Ho·∫∑c 'plasma', 'turbo', 'jet', 'YlOrRd'\n",
    "    alpha=0.75,\n",
    "    vmin=0,\n",
    "    vmax=400\n",
    ")\n",
    "plt.axis('off')\n",
    "plt.savefig(f\"images/heatmap_overlay {idx}.png\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "plt.close()\n",
    "\n",
    "# ==== 4. T·∫°o b·∫£n ƒë·ªì v√† ch√®n ·∫£nh ====\n",
    "\n",
    "# Overlay ·∫£nh l√™n ƒë√∫ng v√πng ƒë·ªãa l√Ω\n",
    "ImageOverlay(\n",
    "    image=f\"images/heatmap_overlay {idx}.png\",\n",
    "    bounds=[[lat_min, lon_min], [lat_max, lon_max]],\n",
    "    opacity=0.6,\n",
    "    interactive=True,\n",
    "    cross_origin=False\n",
    ").add_to(m)\n",
    "\n",
    "    # ƒê·ªçc v√† th√™m bi√™n gi·ªõi VN n·∫øu c√≥\n",
    "try:\n",
    "    folium.GeoJson(\n",
    "        vn_full,\n",
    "        name='Bi√™n gi·ªõi',\n",
    "        style_function=lambda x: {\n",
    "            'fillColor': 'none',\n",
    "            'color': 'black',\n",
    "            'weight': 1,\n",
    "            'dashArray': '5, 5'\n",
    "        }\n",
    "    ).add_to(m)\n",
    "except:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y t·ªáp GeoJSON bi√™n gi·ªõi.\")\n",
    "\n",
    "def get_folium_color(aqi):\n",
    "    if aqi <= 50:\n",
    "        return 'green'\n",
    "    elif aqi <= 100:\n",
    "        return 'beige'      # g·∫ßn gi·ªëng yellow\n",
    "    elif aqi <= 150:\n",
    "        return 'orange'\n",
    "    elif aqi <= 200:\n",
    "        return 'red'\n",
    "    elif aqi <= 300:\n",
    "        return 'purple'\n",
    "    else:\n",
    "        return 'darkred'\n",
    "\n",
    "\n",
    "for name, lat, lon, aqi_value in zip(filtered_df[\"T√™n\"], filtered_df[\"Kinh ƒë·ªô\"], filtered_df[\"Vƒ© ƒë·ªô\"], filtered_df[\"AQI_PM2.5\"]):\n",
    "    # aqi_value = intensity_values[list(coordinates.keys()).index(name)]\n",
    "\n",
    "    fill_color = (\n",
    "        'green'  if aqi_value <= 50 else\n",
    "        'yellow' if aqi_value <= 100 else\n",
    "        'orange' if aqi_value <= 150 else\n",
    "        'red'    if aqi_value <= 200 else\n",
    "        'purple' if aqi_value <= 300 else\n",
    "        'maroon'\n",
    "    )\n",
    "\n",
    "    folium.Marker(\n",
    "        location=(lat, lon),\n",
    "        popup=f\"{name}: AQI PM2.5 = {aqi_value:.1f}\",\n",
    "        icon=folium.Icon(color=get_folium_color(aqi_value), icon=\"info-sign\")  # icon b·∫°n c√≥ th·ªÉ ƒë·ªïi th√†nh 'cloud', 'leaf', 'home'...\n",
    "    ).add_to(m)\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=(lat, lon),\n",
    "        radius=6,\n",
    "        popup=f\"{name}: pm2.5 {aqi_value}\",\n",
    "        color='black',\n",
    "        weight=0.5,\n",
    "        fill=True,\n",
    "        fill_color=fill_color,\n",
    "        fill_opacity=0.8\n",
    "    ).add_to(m)\n",
    "\n",
    "folium.Marker(\n",
    "    location=[16.5053, 111.9537],\n",
    "    icon=folium.DivIcon(\n",
    "        html='<div style=\"font-size: 12px; color: black; font-weight: bold;\">Ho√†ng Sa</div>'\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "folium.Marker(\n",
    "    location=[9.9342, 114.3302],\n",
    "    icon=folium.DivIcon(\n",
    "        html='<div style=\"font-size: 12px; color: black; font-weight: bold;\">Tr∆∞·ªùng Sa</div>'\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "# m.save(f\"heatmap_image_overlay{idx}.html\")\n",
    "print(\"‚úÖ ƒê√£ t·∫°o b·∫£n ƒë·ªì heatmap v·ªõi overlay h√¨nh ·∫£nh: heatmap_image_overlay.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39237ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from branca.element import Template, MacroElement\n",
    "\n",
    "colorbar_template = \"\"\"\n",
    "{% macro html(this, kwargs) %}\n",
    "<div style=\"\n",
    "    position: fixed;\n",
    "    bottom: 50px;\n",
    "    left: 50px;\n",
    "    width: 200px;\n",
    "    height: 20px;\n",
    "    z-index:9999;\n",
    "    background: linear-gradient(to right, green, yellow, orange, red, purple, maroon);\n",
    "    border: 1px solid black;\n",
    "    text-align: center;\n",
    "    font-size: 12px;\n",
    "    color: black;\">\n",
    "    0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;50&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;100&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;150&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;200&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;300&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;500+\n",
    "</div>\n",
    "{% endmacro %}\n",
    "\"\"\"\n",
    "\n",
    "colorbar = MacroElement()\n",
    "colorbar._template = Template(colorbar_template)\n",
    "m.get_root().add_child(colorbar)\n",
    "m.save(f\"heatmap_image_overlaycolorbar.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934d60b",
   "metadata": {},
   "source": [
    "# Ph√¢n t√≠ch d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"So lieu TB 1h T11.2024-T3.2025\"\n",
    "all_dfs = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if not file.endswith((\".xls\", \".xlsx\")):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    location_name = os.path.splitext(file)[0]  # t√™n file l√†m t√™n ƒë·ªãa ƒëi·ªÉm\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        df.columns = df.columns.str.strip()  # x√≥a kho·∫£ng tr·∫Øng\n",
    "        # print(df.columns)\n",
    "        # T√¨m c·ªôt t√™n PM2.5 (t·ª± ƒë·ªông kh·ªõp t√™n g·∫ßn ƒë√∫ng)\n",
    "        pm_col = next((col for col in df.columns if \"pm2.5\" in col.lower() or \"pm-2-5\" in col.lower()), None)\n",
    "        time_col = next((col for col in df.columns if \"time\" in col.lower() or \"datetime\" in col.lower() or \"gi·ªù\" in col.lower()), None)\n",
    "        # temp_col = df[\"Nhi·ªát ƒë·ªô(oC}\"]\n",
    "        print(pm_col)\n",
    "        \n",
    "        if not pm_col or not time_col:\n",
    "            print(f\"‚ö†Ô∏è B·ªè qua: {file} (kh√¥ng t√¨m th·∫•y c·ªôt PM2.5 ho·∫∑c th·ªùi gian)\")\n",
    "            continue\n",
    "\n",
    "        # Ch·ªçn c·ªôt c·∫ßn thi·∫øt\n",
    "        subset = df[[time_col, pm_col]].copy()\n",
    "        subset.columns = [\"Datetime\", \"PM2.5\"]\n",
    "        subset[\"PM2.5\"] = pd.to_numeric(subset[\"PM2.5\"], errors=\"coerce\")\n",
    "        subset[\"PM2.5\"] = subset[\"PM2.5\"].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # G√°n t√™n ƒë·ªãa ƒëi·ªÉm\n",
    "        subset[\"location\"] = location_name\n",
    "\n",
    "        all_dfs.append(subset)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói ƒë·ªçc {file}: {e}\")\n",
    "    # break\n",
    "# G·ªôp to√†n b·ªô\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Chu·∫©n ho√° ƒë·ªãnh d·∫°ng th·ªùi gian\n",
    "# combined_df[\"Datetime\"] = pd.to_datetime(combined_df[\"Datetime\"], errors='coerce')\n",
    "\n",
    "# Xem k·∫øt qu·∫£\n",
    "print(\"‚úÖ S·ªë d√≤ng t·ªïng c·ªông:\", len(combined_df))\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e676e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df[\"Datetime\"] = pd.to_datetime(combined_df[\"Datetime\"], errors='coerce')\n",
    "combined_df.to_csv('data_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a008207",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ed229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_processed.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8569b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi c·ªôt PM2.5 sang d·∫°ng s·ªë, kh√¥ng h·ª£p l·ªá chuy·ªÉn th√†nh NaN\n",
    "df['PM2.5'] = pd.to_numeric(df['PM2.5'], errors='coerce')\n",
    "\n",
    "# Chuy·ªÉn c·ªôt Datetime sang ki·ªÉu datetime\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'], format='%H:%M %d/%m/%Y')\n",
    "\n",
    "# T·∫°o c·ªôt m·ªõi ch·ªâ ch·ª©a gi·ªù\n",
    "df['Hour'] = df['Datetime'].dt.hour\n",
    "df['Day'] = df['Datetime'].dt.day\n",
    "df['Month'] = df['Datetime'].dt.month\n",
    "\n",
    "# K·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['Datetime', 'Hour', 'Day', 'Month', 'PM2.5', 'location']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a56dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"location_id\"] = df[\"location\"].astype(\"category\").cat.codes\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d7d06",
   "metadata": {},
   "source": [
    "# D·ª± ƒêo√°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49383cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "folder_path = \"So lieu TB 1h T11.2024-T3.2025\"  # th∆∞ m·ª•c ch·ª©a file c√°c t·ªânh\n",
    "all_dfs = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\") or file.endswith(\".xlsx\"):\n",
    "        path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # ƒê·ªçc t·ª´ng file\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path, decimal=\",\")\n",
    "        else:\n",
    "            df = pd.read_excel(path)\n",
    "\n",
    "        df.columns = df.columns.str.upper().str.strip().str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
    "        df[\"DATETIME\"] = pd.to_datetime(df[\"DATETIME\"], format=\"%H:%M %d/%m/%Y\", errors='coerce')\n",
    "        df = df.dropna(subset=[\"DATETIME\"])\n",
    "        df[\"LOCATION\"] = file.split(\"_\")[0]  # ho·∫∑c b·∫°n tr√≠ch t·ª´ t√™n c·ªôt kh√°c\n",
    "        all_dfs.append(df)\n",
    "\n",
    "# G·ªôp l·∫°i\n",
    "df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5294085",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"So lieu TB 1h T11.2024-T3.2025/B·∫Øc Giang_ Khu li√™n c∆° quan t·ªânh B·∫Øc Giang - P. Ng√¥ Quy·ªÅn - TP. B·∫Øc Giang (KK)_20250506_101229.xlsx\")\n",
    "# df.columns = df.columns.str.upper().str.strip().str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], format=\"%H:%M %d/%m/%Y\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9689e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def auto_filter_columns(df, threshold=0.2):\n",
    "\n",
    "    missing_ratio = df.isnull().mean()\n",
    "    cols_to_keep = missing_ratio[missing_ratio <= threshold].index\n",
    "    cols_dropped = missing_ratio[missing_ratio > threshold].index.tolist()\n",
    "    \n",
    "    df_filtered = df[cols_to_keep].copy()\n",
    "    \n",
    "    print(f\"‚úÖ Gi·ªØ l·∫°i {len(cols_to_keep)} c·ªôt. üóëÔ∏è Lo·∫°i b·ªè {len(cols_dropped)} c·ªôt:\")\n",
    "    if cols_dropped:\n",
    "        print(\"‚û§ C·ªôt b·ªã lo·∫°i do thi·∫øu qu√° nhi·ªÅu:\")\n",
    "        print(cols_dropped)\n",
    "    \n",
    "    return df_filtered, cols_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb77bc9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04c3a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143822 entries, 0 to 143821\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   Datetime     143822 non-null  object \n",
      " 1   PM2.5        143822 non-null  float64\n",
      " 2   location     143822 non-null  object \n",
      " 3   location_id  143822 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_format.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00169e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['location_id'] = df['location'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197465f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>location</th>\n",
       "      <th>location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00 01/12/2024</td>\n",
       "      <td>125.8175</td>\n",
       "      <td>B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01:00 01/12/2024</td>\n",
       "      <td>122.7142</td>\n",
       "      <td>B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02:00 01/12/2024</td>\n",
       "      <td>111.0983</td>\n",
       "      <td>B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03:00 01/12/2024</td>\n",
       "      <td>87.4533</td>\n",
       "      <td>B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04:00 01/12/2024</td>\n",
       "      <td>70.1417</td>\n",
       "      <td>B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143817</th>\n",
       "      <td>19:00 31/03/2025</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143818</th>\n",
       "      <td>20:00 31/03/2025</td>\n",
       "      <td>0.4071</td>\n",
       "      <td>ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143819</th>\n",
       "      <td>21:00 31/03/2025</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143820</th>\n",
       "      <td>22:00 31/03/2025</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143821</th>\n",
       "      <td>23:00 31/03/2025</td>\n",
       "      <td>0.2123</td>\n",
       "      <td>ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143822 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Datetime     PM2.5  \\\n",
       "0       00:00 01/12/2024  125.8175   \n",
       "1       01:00 01/12/2024  122.7142   \n",
       "2       02:00 01/12/2024  111.0983   \n",
       "3       03:00 01/12/2024   87.4533   \n",
       "4       04:00 01/12/2024   70.1417   \n",
       "...                  ...       ...   \n",
       "143817  19:00 31/03/2025    0.3805   \n",
       "143818  20:00 31/03/2025    0.4071   \n",
       "143819  21:00 31/03/2025    0.2883   \n",
       "143820  22:00 31/03/2025    0.2274   \n",
       "143821  23:00 31/03/2025    0.2123   \n",
       "\n",
       "                                                 location  location_id  \n",
       "0       B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...            0  \n",
       "1       B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...            0  \n",
       "2       B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...            0  \n",
       "3       B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...            0  \n",
       "4       B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp ...            0  \n",
       "...                                                   ...          ...  \n",
       "143817  ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...           54  \n",
       "143818  ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...           54  \n",
       "143819  ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...           54  \n",
       "143820  ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...           54  \n",
       "143821  ƒê√† N·∫µng_ S∆°n Tr√†  Ng≈© H√†nh S∆°n (KK)_20250506_1...           54  \n",
       "\n",
       "[143822 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new = df\n",
    "data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_new.to_csv('data_format.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e28913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=100, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # ch·ªâ l·∫•y output c·ªßa b∆∞·ªõc cu·ªëi\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_point = []\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], format=\"%H:%M %d/%m/%Y\")\n",
    "df.sort_values(\"Datetime\", inplace=True)\n",
    "print(df.columns.tolist())\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].str.replace(',', '.', regex=False)\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ ki·ªÉu float, tr·ª´ c·ªôt th·ªùi gian\n",
    "for col in df.columns:\n",
    "    if col != 'Datetime':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "data = df[[\n",
    "    \"PM2.5\",\n",
    "    \"location_id\",\n",
    "    ]].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# ==== 3. H√†m t·∫°o chu·ªói th·ªùi gian ====\n",
    "def create_sequences(data, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(seq_len, len(data)):\n",
    "        x = data[i-seq_len:i]\n",
    "        y = data[i][0]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "SEQ_LEN = 10  # d√πng 5 b∆∞·ªõc tr∆∞·ªõc ƒë·ªÉ d·ª± ƒëo√°n b∆∞·ªõc ti·∫øp theo\n",
    "X, y = create_sequences(data_scaled, SEQ_LEN)\n",
    "# ==== 4. Chia train/test ====\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "X_train = torch.nan_to_num(X_train)\n",
    "y_train = torch.nan_to_num(y_train)\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = LSTMModel()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        yb = yb.view(-1, 1)  # reshape ƒë·ªÉ kh·ªõp output\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "model_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee45771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0043\n",
      "Epoch 50, Loss: 0.0403\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 29.28 - D·ª± ƒëo√°n: 21.53\n",
      "Th·ª±c t·∫ø: 29.51 - D·ª± ƒëo√°n: 20.99\n",
      "Th·ª±c t·∫ø: 30.20 - D·ª± ƒëo√°n: 20.83\n",
      "Th·ª±c t·∫ø: 28.27 - D·ª± ƒëo√°n: 20.88\n",
      "Th·ª±c t·∫ø: 23.34 - D·ª± ƒëo√°n: 20.75\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0602\n",
      "Epoch 50, Loss: 0.0602\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 7.52 - D·ª± ƒëo√°n: 11.40\n",
      "Th·ª±c t·∫ø: 8.11 - D·ª± ƒëo√°n: 11.45\n",
      "Th·ª±c t·∫ø: 7.28 - D·ª± ƒëo√°n: 11.51\n",
      "Th·ª±c t·∫ø: 12.01 - D·ª± ƒëo√°n: 11.47\n",
      "Th·ª±c t·∫ø: 11.81 - D·ª± ƒëo√°n: 11.76\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0039\n",
      "Epoch 50, Loss: 0.0054\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 6.86 - D·ª± ƒëo√°n: 2.70\n",
      "Th·ª±c t·∫ø: 4.90 - D·ª± ƒëo√°n: 2.79\n",
      "Th·ª±c t·∫ø: 3.05 - D·ª± ƒëo√°n: 2.72\n",
      "Th·ª±c t·∫ø: 4.55 - D·ª± ƒëo√°n: 2.64\n",
      "Th·ª±c t·∫ø: 3.03 - D·ª± ƒëo√°n: 2.69\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0057\n",
      "Epoch 50, Loss: 0.0169\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 34.26 - D·ª± ƒëo√°n: 35.49\n",
      "Th·ª±c t·∫ø: 32.78 - D·ª± ƒëo√°n: 35.70\n",
      "Th·ª±c t·∫ø: 27.82 - D·ª± ƒëo√°n: 35.66\n",
      "Th·ª±c t·∫ø: 32.23 - D·ª± ƒëo√°n: 35.23\n",
      "Th·ª±c t·∫ø: 31.71 - D·ª± ƒëo√°n: 35.45\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0178\n",
      "Epoch 50, Loss: 0.0095\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 6.27 - D·ª± ƒëo√°n: 2.90\n",
      "Th·ª±c t·∫ø: 6.41 - D·ª± ƒëo√°n: 2.94\n",
      "Th·ª±c t·∫ø: 6.22 - D·ª± ƒëo√°n: 2.95\n",
      "Th·ª±c t·∫ø: 6.22 - D·ª± ƒëo√°n: 2.93\n",
      "Th·ª±c t·∫ø: 6.03 - D·ª± ƒëo√°n: 2.92\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0009\n",
      "Epoch 50, Loss: 0.0008\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 18.79 - D·ª± ƒëo√°n: 22.82\n",
      "Th·ª±c t·∫ø: 14.29 - D·ª± ƒëo√°n: 22.09\n",
      "Th·ª±c t·∫ø: 22.07 - D·ª± ƒëo√°n: 21.31\n",
      "Th·ª±c t·∫ø: 23.14 - D·ª± ƒëo√°n: 21.88\n",
      "Th·ª±c t·∫ø: 22.70 - D·ª± ƒëo√°n: 22.47\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0392\n",
      "Epoch 50, Loss: 0.0223\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 7.58 - D·ª± ƒëo√°n: 9.27\n",
      "Th·ª±c t·∫ø: 17.44 - D·ª± ƒëo√°n: 8.76\n",
      "Th·ª±c t·∫ø: 8.38 - D·ª± ƒëo√°n: 9.29\n",
      "Th·ª±c t·∫ø: 1.33 - D·ª± ƒëo√°n: 8.81\n",
      "Th·ª±c t·∫ø: 2.32 - D·ª± ƒëo√°n: 8.22\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0097\n",
      "Epoch 50, Loss: 0.0097\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 5.74 - D·ª± ƒëo√°n: 0.07\n",
      "Th·ª±c t·∫ø: 16.95 - D·ª± ƒëo√°n: 0.07\n",
      "Th·ª±c t·∫ø: 3.41 - D·ª± ƒëo√°n: 0.34\n",
      "Th·ª±c t·∫ø: 1.83 - D·ª± ƒëo√°n: 0.06\n",
      "Th·ª±c t·∫ø: 3.06 - D·ª± ƒëo√°n: -0.06\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0129\n",
      "Epoch 50, Loss: 0.0164\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 1.62 - D·ª± ƒëo√°n: -168.67\n",
      "Th·ª±c t·∫ø: 1.18 - D·ª± ƒëo√°n: -168.67\n",
      "Th·ª±c t·∫ø: 1.63 - D·ª± ƒëo√°n: -168.67\n",
      "Th·ª±c t·∫ø: 1.76 - D·ª± ƒëo√°n: -168.67\n",
      "Th·ª±c t·∫ø: 2.61 - D·ª± ƒëo√°n: -168.67\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0320\n",
      "Epoch 50, Loss: 0.0001\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 2.98 - D·ª± ƒëo√°n: 322.59\n",
      "Th·ª±c t·∫ø: 4.35 - D·ª± ƒëo√°n: 322.59\n",
      "Th·ª±c t·∫ø: 5.12 - D·ª± ƒëo√°n: 322.62\n",
      "Th·ª±c t·∫ø: 6.46 - D·ª± ƒëo√°n: 322.64\n",
      "Th·ª±c t·∫ø: 5.88 - D·ª± ƒëo√°n: 322.66\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0133\n",
      "Epoch 50, Loss: 0.0132\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 12.03 - D·ª± ƒëo√°n: 20.60\n",
      "Th·ª±c t·∫ø: 14.63 - D·ª± ƒëo√°n: 20.71\n",
      "Th·ª±c t·∫ø: 15.24 - D·ª± ƒëo√°n: 21.18\n",
      "Th·ª±c t·∫ø: 15.50 - D·ª± ƒëo√°n: 21.55\n",
      "Th·ª±c t·∫ø: 17.33 - D·ª± ƒëo√°n: 21.69\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0044\n",
      "Epoch 50, Loss: 0.0018\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 32.64 - D·ª± ƒëo√°n: 15.10\n",
      "Th·ª±c t·∫ø: 33.67 - D·ª± ƒëo√°n: 15.15\n",
      "Th·ª±c t·∫ø: 42.47 - D·ª± ƒëo√°n: 15.28\n",
      "Th·ª±c t·∫ø: 46.85 - D·ª± ƒëo√°n: 15.82\n",
      "Th·ª±c t·∫ø: 47.54 - D·ª± ƒëo√°n: 16.16\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0048\n",
      "Epoch 50, Loss: 0.0037\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 57.21 - D·ª± ƒëo√°n: 38.87\n",
      "Th·ª±c t·∫ø: 62.11 - D·ª± ƒëo√°n: 39.03\n",
      "Th·ª±c t·∫ø: 65.97 - D·ª± ƒëo√°n: 39.41\n",
      "Th·ª±c t·∫ø: 66.48 - D·ª± ƒëo√°n: 39.85\n",
      "Th·ª±c t·∫ø: 68.58 - D·ª± ƒëo√°n: 40.03\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0010\n",
      "Epoch 50, Loss: 0.0014\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 50.85 - D·ª± ƒëo√°n: 40.98\n",
      "Th·ª±c t·∫ø: 45.39 - D·ª± ƒëo√°n: 41.62\n",
      "Th·ª±c t·∫ø: 31.25 - D·ª± ƒëo√°n: 41.26\n",
      "Th·ª±c t·∫ø: 22.10 - D·ª± ƒëo√°n: 39.36\n",
      "Th·ª±c t·∫ø: 29.35 - D·ª± ƒëo√°n: 37.34\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0069\n",
      "Epoch 50, Loss: 0.0081\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 46.55 - D·ª± ƒëo√°n: 34.27\n",
      "Th·ª±c t·∫ø: 43.08 - D·ª± ƒëo√°n: 34.52\n",
      "Th·ª±c t·∫ø: 30.67 - D·ª± ƒëo√°n: 34.69\n",
      "Th·ª±c t·∫ø: 29.18 - D·ª± ƒëo√°n: 33.12\n",
      "Th·ª±c t·∫ø: 26.64 - D·ª± ƒëo√°n: 31.94\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0219\n",
      "Epoch 50, Loss: 0.0060\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 55.97 - D·ª± ƒëo√°n: 52.74\n",
      "Th·ª±c t·∫ø: 59.60 - D·ª± ƒëo√°n: 52.73\n",
      "Th·ª±c t·∫ø: 70.60 - D·ª± ƒëo√°n: 53.07\n",
      "Th·ª±c t·∫ø: 77.14 - D·ª± ƒëo√°n: 54.21\n",
      "Th·ª±c t·∫ø: 75.91 - D·ª± ƒëo√°n: 55.28\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0038\n",
      "Epoch 50, Loss: 0.0071\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 82.53 - D·ª± ƒëo√°n: 47.39\n",
      "Th·ª±c t·∫ø: 59.93 - D·ª± ƒëo√°n: 46.10\n",
      "Th·ª±c t·∫ø: 45.86 - D·ª± ƒëo√°n: 43.54\n",
      "Th·ª±c t·∫ø: 52.31 - D·ª± ƒëo√°n: 41.24\n",
      "Th·ª±c t·∫ø: 63.85 - D·ª± ƒëo√°n: 40.92\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0061\n",
      "Epoch 50, Loss: 0.0043\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 43.88 - D·ª± ƒëo√°n: 32.87\n",
      "Th·ª±c t·∫ø: 51.07 - D·ª± ƒëo√°n: 33.93\n",
      "Th·ª±c t·∫ø: 69.16 - D·ª± ƒëo√°n: 35.04\n",
      "Th·ª±c t·∫ø: 55.70 - D·ª± ƒëo√°n: 37.15\n",
      "Th·ª±c t·∫ø: 49.75 - D·ª± ƒëo√°n: 36.53\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0084\n",
      "Epoch 50, Loss: 0.0279\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 2.13 - D·ª± ƒëo√°n: 1.23\n",
      "Th·ª±c t·∫ø: 2.08 - D·ª± ƒëo√°n: 1.23\n",
      "Th·ª±c t·∫ø: 2.14 - D·ª± ƒëo√°n: 1.22\n",
      "Th·ª±c t·∫ø: 2.27 - D·ª± ƒëo√°n: 1.23\n",
      "Th·ª±c t·∫ø: 2.38 - D·ª± ƒëo√°n: 1.23\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0023\n",
      "Epoch 50, Loss: 0.0025\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 2.95 - D·ª± ƒëo√°n: 10.90\n",
      "Th·ª±c t·∫ø: 2.95 - D·ª± ƒëo√°n: 10.88\n",
      "Th·ª±c t·∫ø: 43.25 - D·ª± ƒëo√°n: 10.88\n",
      "Th·ª±c t·∫ø: 30.99 - D·ª± ƒëo√°n: 12.59\n",
      "Th·ª±c t·∫ø: 35.90 - D·ª± ƒëo√°n: 12.87\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0065\n",
      "Epoch 50, Loss: 0.0043\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 19.33 - D·ª± ƒëo√°n: 25.40\n",
      "Th·ª±c t·∫ø: 17.19 - D·ª± ƒëo√°n: 25.00\n",
      "Th·ª±c t·∫ø: 16.30 - D·ª± ƒëo√°n: 24.78\n",
      "Th·ª±c t·∫ø: 16.12 - D·ª± ƒëo√°n: 24.68\n",
      "Th·ª±c t·∫ø: 19.13 - D·ª± ƒëo√°n: 24.63\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0234\n",
      "Epoch 50, Loss: 0.0091\n",
      "\n",
      "üìà D·ª± ƒëo√°n PM2.5:\n",
      "Th·ª±c t·∫ø: 3.13 - D·ª± ƒëo√°n: 12.22\n",
      "Th·ª±c t·∫ø: 1.38 - D·ª± ƒëo√°n: 12.12\n",
      "Th·ª±c t·∫ø: 2.11 - D·ª± ƒëo√°n: 11.99\n",
      "Th·ª±c t·∫ø: 1.96 - D·ª± ƒëo√°n: 12.00\n",
      "Th·ª±c t·∫ø: 3.51 - D·ª± ƒëo√°n: 12.03\n",
      "['Datetime', 'PM2.5', 'location', 'location_id']\n",
      "Epoch 0, Loss: 0.0029\n",
      "Epoch 50, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_point = []\n",
    "\n",
    "for points in range(0, 55):\n",
    "    df = data_new[data_new['location_id'] == points]\n",
    "    df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], format=\"%H:%M %d/%m/%Y\")\n",
    "    # df.sort_values(\"Datetime\", inplace=True)\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.replace(',', '.', regex=False)\n",
    "\n",
    "    # Chuy·ªÉn v·ªÅ ki·ªÉu float, tr·ª´ c·ªôt th·ªùi gian\n",
    "    for col in df.columns:\n",
    "        if col != 'Datetime':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    data = df[[\n",
    "        \"PM2.5\",\n",
    "        # \"location_id\"\n",
    "        ]].values\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    # ==== 3. H√†m t·∫°o chu·ªói th·ªùi gian ====\n",
    "    def create_sequences(data, seq_len):\n",
    "        xs, ys = [], []\n",
    "        for i in range(seq_len, len(data)):\n",
    "            x = data[i-seq_len:i]\n",
    "            y = data[i]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return np.array(xs), np.array(ys)\n",
    "\n",
    "    SEQ_LEN = 24  # d√πng 5 b∆∞·ªõc tr∆∞·ªõc ƒë·ªÉ d·ª± ƒëo√°n b∆∞·ªõc ti·∫øp theo\n",
    "    X, y = create_sequences(data_scaled, SEQ_LEN)\n",
    "    # ==== 4. Chia train/test ====\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "    X_train = torch.nan_to_num(X_train)\n",
    "    y_train = torch.nan_to_num(y_train)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    epochs = 100\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model = LSTMModel()\n",
    "        # ==== 6. Train ====\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            \n",
    "            output = model(xb)\n",
    "            loss = loss_fn(output, yb)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # ==== 7. D·ª± ƒëo√°n ====\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).numpy()\n",
    "        y_test_inv = scaler.inverse_transform(y_test)\n",
    "        y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "    model_point.append(model)\n",
    "    # ==== 8. Hi·ªÉn th·ªã k·∫øt qu·∫£ ====\n",
    "    print(\"\\nüìà D·ª± ƒëo√°n PM2.5:\")\n",
    "    for real, pred in zip(y_test_inv[:5], y_pred_inv[:5]):\n",
    "        print(f\"Th·ª±c t·∫ø: {real[0]:.2f} - D·ª± ƒëo√°n: {pred[0]:.2f}\")\n",
    "model_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615c1dd",
   "metadata": {},
   "source": [
    "# OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import folium\n",
    "import pandas as pd\n",
    "from folium import Map, Marker\n",
    "from io import StringIO\n",
    "\n",
    "df = pd.read_excel(\"output/AQI_PM25_VietNam.xlsx\")\n",
    "df = df[df['Datetime'] == \"12:00 25/01/2025\"]\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# H√†m t·∫°o b·∫£n ƒë·ªì t·ª´ DataFrame\n",
    "def create_map(df):\n",
    "    center = [df[\"Vƒ© ƒë·ªô\"].mean(), df[\"Kinh ƒë·ªô\"].mean()]\n",
    "    m = folium.Map(location=center, zoom_start=12)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row[\"Vƒ© ƒë·ªô\"], row[\"Kinh ƒë·ªô\"]],\n",
    "            popup=f\"{row['T√™n']}<br>PM2.5: {row['AQI_PM2.5']}\"\n",
    "        ).add_to(m)\n",
    "\n",
    "    return m._repr_html_()  # Xu·∫•t HTML c·ªßa b·∫£n ƒë·ªì\n",
    "\n",
    "# H√†m ch√≠nh cho Gradio\n",
    "def show_map_and_data():\n",
    "    map_html = create_map(df)\n",
    "    return map_html, df\n",
    "\n",
    "# T·∫°o giao di·ªán Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üåç B·∫£n ƒë·ªì ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ v√† d·ªØ li·ªáu PM2.5\")\n",
    "    with gr.Row():\n",
    "        map_display = gr.HTML()\n",
    "        data_display = gr.Dataframe(interactive=False)\n",
    "\n",
    "    btn = gr.Button(\"Hi·ªÉn th·ªã b·∫£n ƒë·ªì v√† d·ªØ li·ªáu\")\n",
    "    btn.click(fn=show_map_and_data, outputs=[map_display, data_display])\n",
    "\n",
    "# Ch·∫°y\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Gi·∫£ s·ª≠ models l√† danh s√°ch c√°c LSTMModel\n",
    "save_folder = \"models\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "for idx, model in enumerate(model_point):\n",
    "    torch.save(model, f\"{save_folder}/lstm_model_full_{idx}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "models_loaded = []\n",
    "for i in range(54):  # ho·∫∑c len(models)\n",
    "    model = torch.load(f\"models/lstm_model_full_{i}.pt\")\n",
    "    model.eval()  # ƒë·ªÉ ƒë∆∞a v·ªÅ ch·∫ø ƒë·ªô ƒë√°nh gi√°\n",
    "    models_loaded.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# ==== 3. H√†m t·∫°o chu·ªói th·ªùi gian ====\n",
    "def create_sequences(data, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(seq_len, len(data)):\n",
    "        x = data[i-seq_len:i]\n",
    "        y = data[i][0]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "SEQ_LEN = 10  # d√πng 5 b∆∞·ªõc tr∆∞·ªõc ƒë·ªÉ d·ª± ƒëo√°n b∆∞·ªõc ti·∫øp theo\n",
    "X, y = create_sequences(data_scaled, SEQ_LEN)\n",
    "\n",
    "X.shape, y.shape\n",
    "# ==== 4. Chia train/test ====\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "X_train = torch.nan_to_num(X_train)\n",
    "y_train = torch.nan_to_num(y_train)\n",
    "\n",
    "model = LSTMModel()\n",
    "\n",
    "# ==== 6. Train ====\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    output = model(X_train)\n",
    "    loss = loss_fn(output, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ==== 7. D·ª± ƒëo√°n ====\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).numpy()\n",
    "    y_test_inv = scaler.inverse_transform(y_test)\n",
    "    y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# ==== 8. Hi·ªÉn th·ªã k·∫øt qu·∫£ ====\n",
    "print(\"\\nüìà D·ª± ƒëo√°n PM2.5:\")\n",
    "# listpre = []\n",
    "# listreal = []\n",
    "for real, pred in zip(y_test_inv[:65], y_pred_inv[:65]):\n",
    "    # listreal.append(int(pred[5]))\n",
    "    # listpre.append(int(real[5]))\n",
    "    print(f\"Th·ª±c t·∫ø: {real[0]:.2f} - D·ª± ƒëo√°n: {pred[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d492538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==== 1. Load d·ªØ li·ªáu ====\n",
    "# df = pd.read_excel(r\"So lieu TB 1h T11.2024-T3.2025/B√¨nh D∆∞∆°ng_ s·ªë 593 ƒê·∫°i l·ªô B√¨nh D∆∞∆°ng  P. Hi·ªáp Th√†nh (KK)_20250506_101254.xlsx\")\n",
    "# df.columns = df.columns.str.upper().str.strip().str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], format=\"%H:%M %d/%m/%Y\")\n",
    "df.sort_values(\"Datetime\", inplace=True)\n",
    "print(df.columns.tolist())\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].str.replace(',', '.', regex=False)\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ ki·ªÉu float, tr·ª´ c·ªôt th·ªùi gian\n",
    "for col in df.columns:\n",
    "    if col != 'Datetime':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "\n",
    "# Gi·∫£ s·ª≠ b·∫°n ƒë√£ load d·ªØ li·ªáu v√†o df\n",
    "# df_filtered, dropped_columns = auto_filter_columns(df, threshold=0.2)\n",
    "# df_filtered = df_filtered.interpolate(method='linear', limit_direction='both')\n",
    "# # ==== 2. L·∫•y d·ªØ li·ªáu PM2.5 ====\n",
    "# data = df_filtered.drop(columns=['Datetime']).values\n",
    "data = df[[\n",
    "    # \"Nhi·ªát ƒë·ªô(oC}\",\n",
    "    # \"NOx(¬µg/Nm3}\",\n",
    "    # \"SO2(¬µg/Nm3}\",\n",
    "    # \"O3(¬µg/Nm3}\",\n",
    "    # \"PM-10(¬µg/Nm3}\",\n",
    "    # \"PM-2-5(¬µg/Nm3}\",\n",
    "    \"PM2.5\"\n",
    "    # \"NO(¬µg/Nm3}\",\n",
    "    # \"NO2(¬µg/Nm3}\",\n",
    "    # \"CO(mg/Nm3}\",\n",
    "    # \"√Åp su·∫•t kh√≠ quy·ªÉn(hPa}\",\n",
    "    # \"T·ªëc ƒë·ªô gi√≥(m/s}\"\n",
    "    ]].values\n",
    "# df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# ==== 3. H√†m t·∫°o chu·ªói th·ªùi gian ====\n",
    "def create_sequences(data, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(seq_len, len(data)):\n",
    "        x = data[i-seq_len:i]\n",
    "        y = data[i]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "SEQ_LEN = 10  # d√πng 5 b∆∞·ªõc tr∆∞·ªõc ƒë·ªÉ d·ª± ƒëo√°n b∆∞·ªõc ti·∫øp theo\n",
    "X, y = create_sequences(data_scaled, SEQ_LEN)\n",
    "\n",
    "# ==== 4. Chia train/test ====\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "X_train = torch.nan_to_num(X_train)\n",
    "y_train = torch.nan_to_num(y_train)\n",
    "\n",
    "# ==== 5. M√¥ h√¨nh LSTM ====\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=100, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # ch·ªâ l·∫•y output c·ªßa b∆∞·ªõc cu·ªëi\n",
    "        return out\n",
    "\n",
    "model = LSTMModel()\n",
    "\n",
    "# ==== 6. Train ====\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    output = model(X_train)\n",
    "    loss = loss_fn(output, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ==== 7. D·ª± ƒëo√°n ====\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).numpy()\n",
    "    y_test_inv = scaler.inverse_transform(y_test)\n",
    "    y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# ==== 8. Hi·ªÉn th·ªã k·∫øt qu·∫£ ====\n",
    "print(\"\\nüìà D·ª± ƒëo√°n PM2.5:\")\n",
    "# listpre = []\n",
    "# listreal = []\n",
    "for real, pred in zip(y_test_inv[:65], y_pred_inv[:65]):\n",
    "    # listreal.append(int(pred[5]))\n",
    "    # listpre.append(int(real[5]))\n",
    "    print(f\"Th·ª±c t·∫ø: {real[0]:.2f} - D·ª± ƒëo√°n: {pred[0]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
